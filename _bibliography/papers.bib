@article{balepur2023not,
  abbr= "Preprint",
  title={It’s Not Easy Being Wrong: Evaluating Process of Elimination Reasoning in Large Language Models},
  author={Balepur, Nishant and Palta, Shramay and Rudinger, Rachel},
  journal={arXiv preprint},
  year={2023},
  selected={true},
  bibtex_show={true},
  url = "",
  pdf = "",
  abstract = "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward \emph{correct} answers, but its efficacy in reasoning toward \emph{incorrect} answers is unexplored. This strategy of process of elimination (PoE), when used with COT, has the potential to enhance interpretability in tasks like medical diagnoses of exclusion. Thus, we propose PoE with COT, a new task where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on 2-choice commonsense and scientific reasoning datasets. We show that PoE consistently underperforms directly choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct an error analysis and give suggestions for future work.",
}

@inproceedings{balepur2023expository,
  abbr= "EMNLP 2023",
  title={Expository Text Generation: Imitate, Retrieve, Paraphrase},
  author={Balepur, Nishant and Huang, Jie and Chang, Kevin Chen-Chuan},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023},
  year={2023},
  selected={true},
  bibtex_show={true},
  url = "https://arxiv.org/pdf/2305.03276.pdf",
  pdf = "https://arxiv.org/pdf/2305.03276.pdf",
  video = "https://youtu.be/6elNaka-JKM",
  abstract = "Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository documents by hand is a time-consuming and labor-intensive process that requires knowledge of the domain of interest, careful content planning, and the ability to synthesize information from multiple sources. To ease these burdens, we introduce the task of expository text generation, which seeks to automatically generate an accurate and informative expository document from a knowledge source. We solve our task by developing IRP, an iterative framework that overcomes the limitations of language models and separately tackles the steps of content planning, fact selection, and rephrasing. Through experiments on three diverse datasets, we demonstrate that IRP produces high-quality expository documents that accurately inform readers.",
}

@inproceedings{balepur2023fact,
  abbr= "EMNLP 2023",
  title={Text Fact Transfer},
  author={Balepur, Nishant and Huang, Jie and Chang, Kevin Chen-Chuan},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023},
  year={2023},
  selected={true},
  bibtex_show={true},
  url = "https://arxiv.org/pdf/2310.14486.pdf",
  pdf = "https://arxiv.org/pdf/2310.14486.pdf",
  video = "https://youtu.be/U01fVWUbIQw",
  abstract = "Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors. To address these issues, we design ModQGA, a framework that minimally modifies a source text with a novel combination of end-to-end question generation and specificity-aware question answering. Through experiments on four existing datasets adapted for text fact transfer, we show that ModQGA can accurately transfer factual content without sacrificing the style of the source text.",
}

@inproceedings{balepur2023dynamite,
  abbr={ACL 2023},
  title={DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance},
  author={Balepur, Nishant and Agarwal, Shivam and Ramanan, Karthik Venkat and Yoon, Susik and Yang, Diyi and Han, Jiawei},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={194--217},
  year={2023},
  selected={true},
  bibtex_show={true},
  url = "https://aclanthology.org/2023.findings-acl.14.pdf",
  pdf = "https://aclanthology.org/2023.findings-acl.14.pdf",
  video = "https://youtu.be/KAyd-QqYO6Y",
  abstract = "Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user’s needs. Further, the topic evolutions produced by DTMs tend to contain generic terms that are not indicative of their designated time steps. To address these issues, we propose the task of discriminative dynamic topic discovery. This task aims to discover topic evolutions from temporal corpora that distinctly align with a set of user-provided category names and uniquely capture topics at each time step. We solve this task by developing DynaMiTE, a framework that ensembles semantic similarity, category indicative, and time indicative scores to produce informative topic evolutions. Through experiments on three diverse datasets, including the use of a newly-designed human evaluation experiment, we demonstrate that DynaMiTE is a practical and efficient framework for helping users discover high-quality topic evolutions suited to their interests.",
}

@article{balepur2023mastering,
  abbr= "Preprint",
  title={Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation},
  author={Balepur, Nishant and Huang, Jie and Moorjani, Samraj and Sundaram, Hari and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint},
  year={2023},
  bibtex_show={true},
  url = "https://arxiv.org/pdf/2305.14750.pdf",
  pdf = "https://arxiv.org/pdf/2305.14750.pdf",
  abstract = "When answering complex questions, large language models (LLMs) may produce answers that do not satisfy all criteria of the question. While existing self-evaluation techniques aim to detect if such answers are correct, these techniques are unable to determine which criteria of the question are satisfied by the generated answers. To address this issue, we propose answer-based claim decomposition (ABCD), a prompting strategy that decomposes questions into a series of true/false claims that can be used to verify which criteria of the input question an answer satisfies. Using the decomposed ABCD claims, we perform fine-grained self-evaluation. Through preliminary experiments on three datasets, including a newly-collected challenge dataset ObscureQA, we find that GPT-3.5 has some ability to determine to what extent its answer satisfies the criteria of the input question, and can give insights into the errors and knowledge gaps of the model.",
}
